{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The Capstone project analyses and process data from Brazilian stock market since 2013 until now. Also,\n",
    "the Brazilian economic data from [World Bank](https://data.worldbank.org/).\n",
    "\n",
    "With those data in mind, it is possible to evaluate and take insights, relate country data with stock market.\n",
    "Basically to answer questions like:\n",
    "* Does the stock market influence real economy?\n",
    "* Is the population's education rate related to the increase in GDP?\n",
    "* The raising of company in stock market help us somehow?\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bovespa in ./env/lib/python3.8/site-packages (0.1.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\r\n",
      "You should consider upgrading via the '/home/charl3ff/workspace/udacity/capstone-udacity-data-engineer/env/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: pyspark in ./env/lib/python3.8/site-packages (3.1.1)\r\n",
      "Requirement already satisfied: py4j==0.10.9 in ./env/lib/python3.8/site-packages (from pyspark) (0.10.9)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\r\n",
      "You should consider upgrading via the '/home/charl3ff/workspace/udacity/capstone-udacity-data-engineer/env/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: psycopg2 in ./env/lib/python3.8/site-packages (2.8.6)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\r\n",
      "You should consider upgrading via the '/home/charl3ff/workspace/udacity/capstone-udacity-data-engineer/env/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: wbgapi in ./env/lib/python3.8/site-packages (1.0.5)\r\n",
      "Requirement already satisfied: tabulate in ./env/lib/python3.8/site-packages (from wbgapi) (0.8.9)\r\n",
      "Requirement already satisfied: PyYAML in ./env/lib/python3.8/site-packages (from wbgapi) (5.4.1)\r\n",
      "Requirement already satisfied: requests in ./env/lib/python3.8/site-packages (from wbgapi) (2.25.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./env/lib/python3.8/site-packages (from requests->wbgapi) (1.26.4)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./env/lib/python3.8/site-packages (from requests->wbgapi) (4.0.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.8/site-packages (from requests->wbgapi) (2020.12.5)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./env/lib/python3.8/site-packages (from requests->wbgapi) (2.10)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\r\n",
      "You should consider upgrading via the '/home/charl3ff/workspace/udacity/capstone-udacity-data-engineer/env/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: matplotlib in ./env/lib/python3.8/site-packages (3.4.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./env/lib/python3.8/site-packages (from matplotlib) (1.3.1)\r\n",
      "Requirement already satisfied: numpy>=1.16 in ./env/lib/python3.8/site-packages (from matplotlib) (1.20.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./env/lib/python3.8/site-packages (from matplotlib) (2.4.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./env/lib/python3.8/site-packages (from matplotlib) (2.8.1)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./env/lib/python3.8/site-packages (from matplotlib) (8.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./env/lib/python3.8/site-packages (from matplotlib) (0.10.0)\r\n",
      "Requirement already satisfied: six in ./env/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\r\n",
      "You should consider upgrading via the '/home/charl3ff/workspace/udacity/capstone-udacity-data-engineer/env/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: configparser in ./env/lib/python3.8/site-packages (5.0.2)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\r\n",
      "You should consider upgrading via the '/home/charl3ff/workspace/udacity/capstone-udacity-data-engineer/env/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: pyarrow in ./env/lib/python3.8/site-packages (4.0.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.6 in ./env/lib/python3.8/site-packages (from pyarrow) (1.20.2)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\r\n",
      "You should consider upgrading via the '/home/charl3ff/workspace/udacity/capstone-udacity-data-engineer/env/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bovespa\n",
    "!pip install pyspark\n",
    "!pip install psycopg2\n",
    "!pip install wbgapi\n",
    "!pip install matplotlib\n",
    "!pip install configparser\n",
    "!pip install pyarrow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import bovespa\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import wbgapi as wb\n",
    "import configparser\n",
    "from pyspark.sql.functions import col, collect_set, sum, avg, max, countDistinct, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope\n",
    "The Capstone project analyses and process data from Brazilian stock market since 2013 until now. Also,\n",
    "the Brazilian economic data from [World Bank](https://data.worldbank.org/).\n",
    "\n",
    "#### Describe and Gather Data\n",
    "Describe the data sets you're using. Where did it come from? What type of information is included?\n",
    "\n",
    "Bovespa is the main Brazilian stock Exchange. Here is listed all trading data since 2013.\n",
    "\n",
    "It includes the day of the trading, company name, stock id, stock data, such as: price open, price close, price high, price low.\n",
    "\n",
    "World Bank collects all data from all country in the world, since 1960.\n",
    "It has information about environment, education, GDP, economy, social.\n",
    "http://www.b3.com.br/en_us/market-data-and-indices/data-services/market-data/historical-data/equities/historical-quotes/\n",
    "https://api.worldbank.org/v2/en/country/BRA?downloadformat=csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "os.environ['JAVA_HOME'] = '/home/charl3ff/.sdkman/candidates/java/current'\n",
    "spark = SparkSession.builder.enableHiveSupport()\\\n",
    "    .appName(\"Capstone\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .config('spark.sql.execution.arrow.pyspark.enabled', \"true\").getOrCreate()\n",
    "\n",
    "hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", os.environ['AWS_ACCESS_KEY_ID'])\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "hadoop_conf.set(\"fs.s3a.multipart.size\", '104857600')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ETL stock market"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o42.load.\n: java.lang.IllegalArgumentException\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1293)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1215)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:280)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:240)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-87654ffa1b62>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mtrading_file\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minput_data\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\"COTAHIST_*.txt\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrading_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0mdf_pd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoPandas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/workspace/udacity/capstone-udacity-data-engineer/env/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    202\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 204\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    205\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mpath\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    206\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/workspace/udacity/capstone-udacity-data-engineer/env/lib/python3.8/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/workspace/udacity/capstone-udacity-data-engineer/env/lib/python3.8/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    109\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 111\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    112\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/workspace/udacity/capstone-udacity-data-engineer/env/lib/python3.8/site-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o42.load.\n: java.lang.IllegalArgumentException\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1293)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1215)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:280)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:240)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# Load data trading data from bovespa\n",
    "\n",
    "input_data = \"s3a://capstone-data-1/\"\n",
    "trading_file = input_data + \"COTAHIST_*.txt\"\n",
    "\n",
    "df = spark.read.load(trading_file)\n",
    "df_pd = df.toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Parse data\n",
    "\n",
    "def record_to_dict(record):\n",
    "    try:\n",
    "        record = bovespa.Record(record)\n",
    "    except:\n",
    "        return None\n",
    "    return {\n",
    "        'date': record.date, 'year': record.date.year, 'month': record.date.month, 'day': record.date.day,\n",
    "        'money_volume': record.volume, 'volume': record.quantity,\n",
    "        'stock_code': record.stock_code,\n",
    "        'company_name': record.company_name, 'price_open': record.price_open,\n",
    "        'price_close': record.price_close, 'price_mean': record.price_mean,\n",
    "        'price_high': record.price_high, 'price_low': record.price_low,\n",
    "        'variation': None\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load trading data into our schema\n",
    "\n",
    "df_pd['dict'] = df_pd['value'].apply(lambda x: record_to_dict(x))\n",
    "new_df = df_pd['dict'].apply(pd.Series).replace('', None)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('date', DateType(), True),\n",
    "    StructField('year', IntegerType(), True),\n",
    "    StructField('month', IntegerType(), True),\n",
    "    StructField('day', IntegerType(), True),\n",
    "    StructField('money_volume', FloatType(), True),\n",
    "    StructField('volume', IntegerType(), True),\n",
    "    StructField('stock_code', StringType(), True),\n",
    "    StructField('company_name', StringType(), True),\n",
    "    StructField('price_open', FloatType(), True),\n",
    "    StructField('price_close', FloatType(), True),\n",
    "    StructField('price_mean', FloatType(), True),\n",
    "    StructField('price_high', FloatType(), True),\n",
    "    StructField('price_low', FloatType(), True),\n",
    "    StructField('variation', FloatType(), True)\n",
    "])\n",
    "\n",
    "trading_df = spark.createDataFrame(new_df, schema=schema)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set variation column\n",
    "\n",
    "trading_df = trading_df.withColumn('variation', (col('price_close') - col('price_open')) / 100)\n",
    "\n",
    "trading_df.show(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### So, we can take all insights from it.\n",
    "Such as:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Most traded stocks in terms of money\n",
    "trading_df.orderBy('money_volume', ascending=False).show(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Most up variation in a day\n",
    "trading_df.orderBy('variation', ascending=False).show(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ETL World Bank\n",
    "\n",
    "The `wbgapi` package is the official Python/R library to interact with the [World Bank open data](https://data.worldbank.org/),\n",
    "it is useful to easily retrieve data, instead of download it ourselves.\n",
    "\n",
    "The purpose is to fetch data from Brazil, then it will be possible to compare the stock market and the real economy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Metrics related to gdp\n",
    "wb.series.info(q='gdp')\n",
    "\n",
    "# Metrics related to debt, and so on\n",
    "wb.series.info(q='debt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "metric_to_schema = {\n",
    "    'GC.DOD.TOTL.GD.ZS': 'debt',\n",
    "    'GC.XPN.TOTL.CN': 'total_expense',\n",
    "    'NY.GDP.MKTP.KD.ZG': 'gdp_annual_growth',\n",
    "    'NY.GDP.MKTP.CD': 'gdp',\n",
    "    'NY.GDP.PCAP.CD': 'gdp_per_capita',\n",
    "    'SP.POP.TOTL': 'population',\n",
    "    'SP.DYN.LE00.IN': 'life_expectancy',\n",
    "    'GC.XPN.TOTL.GD.ZS': 'expense_per_gdp',\n",
    "    'FI.RES.TOTL.CD': 'total_reserves',\n",
    "    'SE.ADT.LITR.ZS': 'pop_literacy_rate',\n",
    "    'SE.XPD.TOTL.GD.ZS': 'expenditure_education_per_gdp',\n",
    "    'BX.KLT.DINV.CD.WD': 'foreign_investment',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data into pandas\n",
    "country = 'BRA'\n",
    "values = []\n",
    "for key, value in metric_to_schema.items():\n",
    "    df = wb.data.DataFrame(key, economy=country, columns='time', numericTimeKeys=True).dropna(axis=1, how='all')\n",
    "    for data in df.items():\n",
    "        values.append((data[0], value, data[1][country]))\n",
    "economy_df = pd.DataFrame(data=values, columns=['year', 'table', 'value'])\n",
    "economy_df = spark.createDataFrame(economy_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "### Trading Table\n",
    "\n",
    "| Feature        | Description  |\n",
    "| ------------- |:-------------:|\n",
    "| date         | date that stock is being traded                      |\n",
    "| year         | year that stock is being traded                      |\n",
    "| month        | month that stock is being traded                     |\n",
    "| day          | day that stock is being traded                       |\n",
    "| money_volume | total amount of money in all transactions            |\n",
    "| volume       | total number traded stocks                           |\n",
    "| stock_code   | unique stock code                                    |\n",
    "| company_name | name of the company that is represented by the stock |\n",
    "| price_open   | price when trading opened                            |\n",
    "| price_close  | price when trading closed                            |\n",
    "| price_mean   | price mean of the stock                              |\n",
    "| price_high   | price high of the stock                              |\n",
    "| price_low    | price low of the stock                               |\n",
    "| variation    | the variation between price close and price open     |\n",
    "\n",
    "### Economy Table\n",
    "\n",
    "| Feature        | Description  |\n",
    "| ------------- |:-------------:|\n",
    "| year  | the year the statics was collected           |\n",
    "| table | the statics itself, like, GDP, debt over GDP |\n",
    "| value | the value of the statics                     |\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}